"""
Binance Data Fetcher
Downloads historical klines (candlestick data) and saves as Parquet.
Run once, then backtester reads from local files.
"""

import requests
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
import time

# Config
DATA_DIR = Path(__file__).parent / "data" / "raw"
BASE_URL = "https://api.binance.com/api/v3/klines"


def fetch_klines(
    symbol: str,
    interval: str = "1h",
    start_time: datetime = None,
    end_time: datetime = None,
    limit: int = 1000
) -> pd.DataFrame:
    """
    Fetch klines from Binance API.
    
    Args:
        symbol: Trading pair (e.g., "BTCUSDT")
        interval: Candle interval (1m, 5m, 15m, 1h, 4h, 1d)
        start_time: Start datetime
        end_time: End datetime
        limit: Max candles per request (max 1000)
    """
    params = {
        "symbol": symbol,
        "interval": interval,
        "limit": limit
    }
    
    if start_time:
        params["startTime"] = int(start_time.timestamp() * 1000)
    if end_time:
        params["endTime"] = int(end_time.timestamp() * 1000)
    
    response = requests.get(BASE_URL, params=params)
    response.raise_for_status()
    
    data = response.json()

    df = pd.DataFrame(data, columns=[
        "open_time", "open", "high", "low", "close", "volume",
        "close_time", "quote_volume", "trades", "taker_buy_base",
        "taker_buy_quote", "ignore"
    ])
    
    # Clean up
    df["timestamp"] = pd.to_datetime(df["open_time"], unit="ms")
    df["close"] = df["close"].astype(float)
    df["volume"] = df["volume"].astype(float)
    
    return df[["timestamp", "close", "volume"]]


def fetch_full_history(
    symbol: str,
    interval: str = "1h",
    days_back: int = 365
) -> pd.DataFrame:
    """
    Fetch complete history by paginating through API.
    """
    all_data = []
    end_time = datetime.now()
    start_time = end_time - timedelta(days=days_back)
    
    current_start = start_time
    
    print(f"Fetching {symbol} ({interval}) from {start_time.date()} to {end_time.date()}")
    
    while current_start < end_time:
        df = fetch_klines(
            symbol=symbol,
            interval=interval,
            start_time=current_start,
            end_time=end_time,
            limit=1000
        )
        
        if df.empty:
            break
            
        all_data.append(df)
        
        # Move start to after last candle
        current_start = df["timestamp"].max() + timedelta(milliseconds=1)
        
        # Rate limiting - Binance allows 1200 requests/min
        time.sleep(0.1)
        
        print(f"  Fetched {len(df)} candles, up to {df['timestamp'].max()}")
    
    if not all_data:
        return pd.DataFrame()
    
    result = pd.concat(all_data, ignore_index=True)
    result = result.drop_duplicates(subset=["timestamp"]).sort_values("timestamp")
    
    return result


def save_to_parquet(df: pd.DataFrame, symbol: str):
    """Save dataframe to parquet file."""
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    
    filepath = DATA_DIR / f"{symbol}.parquet"
    df.to_parquet(filepath, index=False)
    
    print(f"Saved {len(df)} rows to {filepath}")


def fetch_and_save(symbols: list, interval: str = "1h", days_back: int = 365):
    for symbol in symbols:
        try:
            df = fetch_full_history(symbol, interval, days_back)
            if not df.empty:
                save_to_parquet(df, symbol)
            print()
        except Exception as e:
            print(f"Error fetching {symbol}: {e}")
            continue


def main():
    # Config - start small
    symbols = [
    "BTCUSDT", "ETHUSDT", "SOLUSDT", "AVAXUSDT", "BNBUSDT",
    "ADAUSDT", "DOTUSDT", "MATICUSDT", "LINKUSDT", "ATOMUSDT",
    "XRPUSDT", "DOGEUSDT", "LTCUSDT", "UNIUSDT", "AAVEUSDT",
    "NEARUSDT", "APTUSDT", "ARBUSDT", "OPUSDT", "INJUSDT",
    ]

    fetch_and_save(
        symbols=symbols,
        interval="1h",       # hourly â€” fast download
        days_back=365 * 2    # 2 years
    )
    
    # Summary
    print("="*50)
    print("DOWNLOAD COMPLETE")
    print("="*50)
    
    for f in DATA_DIR.glob("*.parquet"):
        df = pd.read_parquet(f)
        print(f"{f.stem}: {len(df):,} rows, {df['timestamp'].min().date()} to {df['timestamp'].max().date()}")

if __name__ == "__main__":
    main()